/home/saeid/KERN
~~~~~~~~ Hyperparameters used: ~~~~~~~
ckpt : checkpoints/kern_sgcls_predcls.tar
save_dir : 
num_gpus : 1
num_workers : 1
lr : 0.001
batch_size : 1
val_size : 2
l2 : 0.0001
adamwd : 0.0
clip : 5.0
print_interval : 100
mode : sgdet
cache : caches/kern_predcls.pkl
adam : False
test : True
num_epochs : 50
use_resnet : False
use_proposals : False
pooling_dim : 4096
use_ggnn_obj : False
ggnn_obj_time_step_num : 3
ggnn_obj_hidden_dim : 512
ggnn_obj_output_dim : 512
use_obj_knowledge : False
obj_knowledge : ../prior_matrices/obj_matrix.npy
use_ggnn_rel : True
ggnn_rel_time_step_num : 3
ggnn_rel_hidden_dim : 512
ggnn_rel_output_dim : 512
use_rel_knowledge : True
rel_knowledge : ../prior_matrices/rel_matrix.npy
tb_log_dir : 
save_rel_recall : results/kern_rel_recall_sgdet.pkl
../prior_matrices/rel_matrix.npy
mode is:
train
mode is:
test
mode is:
val
mode is:
test
mode is:
test
mode is:
test
../prior_matrices/rel_matrix.npy
torch.Size([1, 3, 592, 592])
Unexpected key ggnn_obj_reason.obj_proj.weight in state_dict with size torch.Size([512, 4096])
Unexpected key ggnn_obj_reason.obj_proj.bias in state_dict with size torch.Size([512])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_eq3_w.weight in state_dict with size torch.Size([512, 1024])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_eq3_w.bias in state_dict with size torch.Size([512])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_eq3_u.weight in state_dict with size torch.Size([512, 512])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_eq3_u.bias in state_dict with size torch.Size([512])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_eq4_w.weight in state_dict with size torch.Size([512, 1024])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_eq4_w.bias in state_dict with size torch.Size([512])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_eq4_u.weight in state_dict with size torch.Size([512, 512])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_eq4_u.bias in state_dict with size torch.Size([512])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_eq5_w.weight in state_dict with size torch.Size([512, 1024])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_eq5_w.bias in state_dict with size torch.Size([512])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_eq5_u.weight in state_dict with size torch.Size([512, 512])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_eq5_u.bias in state_dict with size torch.Size([512])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_output.weight in state_dict with size torch.Size([512, 1024])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_output.bias in state_dict with size torch.Size([512])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_obj_cls.weight in state_dict with size torch.Size([151, 77312])
Unexpected key ggnn_obj_reason.ggnn_obj.fc_obj_cls.bias in state_dict with size torch.Size([151])
dict_keys(['img', 'img_size', 'gt_boxes', 'gt_classes', 'gt_relations', 'scale', 'index', 'flipped', 'fn'])
dict_keys(['img', 'img_size', 'gt_boxes', 'gt_classes', 'gt_relations', 'scale', 'index', 'flipped', 'fn'])
0
['__class__', '__delattr__', '__dict__', '__dir__', '__doc__', '__eq__', '__format__', '__ge__', '__getattribute__', '__getitem__', '__gt__', '__hash__', '__init__', '__init_subclass__', '__le__', '__lt__', '__module__', '__ne__', '__new__', '__reduce__', '__reduce_ex__', '__repr__', '__setattr__', '__sizeof__', '__str__', '__subclasshook__', '__weakref__', '_chunkize', '_scatter', 'all_anchor_inds', 'all_anchors', 'anchor_chunks', 'append', 'batch_size', 'batch_size_per_gpu', 'gt_box_chunks', 'gt_boxes', 'gt_classes', 'gt_nodes', 'gt_rel_chunks', 'gt_rels', 'gt_sents', 'im_sizes', 'imgs', 'is_flickr', 'is_rel', 'is_train', 'mode', 'num_gpus', 'primary_gpu', 'proposal_chunks', 'proposals', 'reduce', 'scatter', 'sent_lengths', 'train_anchor_inds', 'train_anchor_labels', 'train_anchors', 'train_chunks', 'volatile']
<class 'numpy.ndarray'>
[[[444.    592.      1.184]]]
[444.    592.      1.184]
boxes_i
[[1.567e+02 2.409e+02 4.291e+02 5.348e+02]
 [6.899e+01 2.630e+02 1.807e+02 4.329e+02]
 [3.671e+02 6.569e+01 4.338e+02 2.322e+02]
 [1.301e+02 1.163e+02 2.534e+02 2.406e+02]
 [9.980e-01 1.731e+02 5.393e+01 2.740e+02]
 [2.300e+02 1.578e+02 3.640e+02 2.412e+02]
 [0.000e+00 1.072e+02 9.484e+01 1.396e+02]
 [2.273e+02 1.657e+02 3.487e+02 1.983e+02]
 [1.745e+02 4.110e+02 2.481e+02 5.257e+02]
 [0.000e+00 9.385e+01 1.208e+02 2.540e+02]
 [4.783e+01 3.311e+02 6.854e+01 3.923e+02]
 [2.686e+02 2.432e+02 4.375e+02 3.511e+02]
 [4.157e+01 2.512e+02 2.886e+02 4.697e+02]
 [1.991e+02 9.718e+01 2.372e+02 2.268e+02]
 [1.323e+00 2.945e+02 4.374e+02 5.842e+02]
 [7.018e+01 2.660e+02 1.096e+02 2.960e+02]
 [1.559e+02 2.618e+02 3.854e+02 3.373e+02]
 [2.499e-01 9.724e+01 1.110e+02 2.697e+02]
 [3.334e+02 2.518e+02 4.232e+02 3.022e+02]
 [1.009e+02 2.705e+02 1.624e+02 3.339e+02]
 [1.714e+02 4.096e+02 2.435e+02 5.157e+02]
 [1.067e+01 9.024e+01 3.838e+02 2.623e+02]
 [1.778e+02 2.567e+02 3.951e+02 3.277e+02]
 [1.093e+00 8.263e+01 3.717e+02 2.056e+02]
 [9.128e+01 2.104e+02 1.625e+02 2.471e+02]
 [3.220e+02 2.482e+02 4.256e+02 3.366e+02]
 [2.493e+02 3.872e+02 3.294e+02 4.314e+02]
 [2.248e+00 9.014e+01 1.241e+02 1.835e+02]
 [7.126e+01 7.273e+01 3.962e+02 2.269e+02]
 [1.673e+02 1.256e+02 3.589e+02 1.950e+02]
 [2.092e+02 1.528e+02 3.651e+02 2.424e+02]
 [1.962e+02 2.613e+02 4.118e+02 4.687e+02]
 [2.817e+02 2.513e+02 3.374e+02 2.791e+02]
 [1.914e+02 2.424e+02 4.362e+02 4.139e+02]
 [1.314e+01 8.712e+01 3.802e+02 2.106e+02]
 [2.644e+02 4.089e+01 4.384e+02 3.159e+02]
 [2.757e+02 4.943e+02 4.058e+02 5.368e+02]
 [9.190e+01 2.105e+02 1.548e+02 2.242e+02]
 [4.096e+02 3.590e+02 4.414e+02 4.135e+02]
 [7.299e+00 8.190e+01 2.162e+02 1.398e+02]
 [3.995e+02 1.557e+02 4.379e+02 2.329e+02]
 [2.587e+02 2.374e+02 4.304e+02 2.609e+02]
 [2.512e+02 3.885e+02 4.002e+02 4.647e+02]
 [7.798e-01 3.765e+02 4.077e+02 5.834e+02]
 [1.837e+02 1.766e+02 4.307e+02 5.558e+02]
 [2.742e+02 2.510e+02 3.422e+02 3.077e+02]
 [2.534e+02 2.370e+02 4.106e+02 2.664e+02]
 [2.550e+02 1.561e+02 3.102e+02 1.813e+02]
 [6.613e+00 1.677e+02 6.019e+01 2.509e+02]
 [1.966e+02 4.152e+02 4.163e+02 5.385e+02]
 [2.819e+02 4.868e+02 3.966e+02 5.239e+02]
 [2.780e+02 4.025e+02 3.118e+02 4.352e+02]
 [2.561e+02 3.954e+02 4.311e+02 5.043e+02]
 [4.183e+01 2.477e+02 3.248e+02 4.840e+02]
 [2.278e+02 1.298e+02 3.163e+02 1.824e+02]
 [1.304e+02 1.772e+02 4.430e+02 3.305e+02]
 [2.484e+02 2.620e+02 4.276e+02 4.189e+02]
 [2.674e+02 3.947e+02 3.709e+02 4.453e+02]
 [1.523e+01 1.637e+02 7.609e+01 2.641e+02]
 [1.546e+00 2.875e+02 1.284e+02 5.717e+02]
 [2.417e+02 3.641e+02 4.300e+02 4.892e+02]
 [2.062e+02 9.599e+01 2.330e+02 1.468e+02]
 [1.341e+02 1.515e+02 2.042e+02 2.193e+02]
 [2.446e+02 1.495e+02 2.853e+02 1.784e+02]]
(64, 4)
objs_i
[ 26  38 136 136 136  65 105 105 130  22 130  26  26 136 124 145 146  65
 145 145 144  65 145  81  22  38  76 105 136 105  22  38 145 142 105 136
 144 105 136  81 136 105  76 114 137  38  26 105  73 130  97  76  59 142
 136  65 145 145  22 124  26 136  73 136]
64
obj_scores_i
[0.933 0.841 0.833 0.813 0.802 0.798 0.768 0.7   0.611 0.56  0.514 0.505
 0.499 0.497 0.473 0.472 0.462 0.457 0.436 0.41  0.406 0.391 0.325 0.316
 0.297 0.223 0.214 0.213 0.204 0.188 0.13  0.115 0.111 0.097 0.084 0.083
 0.083 0.08  0.08  0.075 0.074 0.072 0.071 0.069 0.068 0.063 0.06  0.052
 0.05  0.049 0.049 0.047 0.047 0.046 0.045 0.045 0.043 0.042 0.039 0.034
 0.033 0.032 0.032 0.031]
64
rels_i
[[ 0 14]
 [ 7  5]
 [12 14]
 ...
 [43 51]
 [49 50]
 [55 53]]
(1070, 2)
pred_scores_i
[[6.292e-01 4.986e-03 3.731e-04 ... 5.081e-06 5.560e-06 5.258e-05]
 [6.942e-01 2.736e-02 2.410e-05 ... 7.685e-05 6.265e-05 5.131e-04]
 [4.251e-01 8.731e-03 5.981e-04 ... 1.948e-05 2.219e-05 4.952e-05]
 ...
 [9.998e-01 3.030e-06 2.446e-06 ... 2.947e-06 4.586e-07 1.174e-05]
 [9.995e-01 8.025e-06 8.308e-06 ... 1.642e-05 4.277e-06 1.957e-05]
 [9.998e-01 7.851e-06 2.523e-06 ... 1.456e-06 7.118e-07 2.526e-06]]
(1070, 51)
